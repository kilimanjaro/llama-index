{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex 워크플로우 절차\n",
    "\n",
    "**작성일**: 2025년 8월 26일\n",
    "\n",
    "최종 정리된 절차\n",
    "```\n",
    "파일 준비: .txt, .pdf, .doc 등 파일을 디렉토리에 준비.\n",
    "파일 읽기: SimpleDirectoryReader로 파일을 읽어 Document 객체 생성.\n",
    "텍스트 분할 및 노드 생성: SimpleNodeParser, SentenceSplitter 등으로 문서를 청크 단위로 분할하여 Node 객체 생성.\n",
    "인덱스 생성: VectorStoreIndex로 노드 또는 문서를 벡터화하고 인덱스 생성.\n",
    "쿼리 엔진 생성: index.as_query_engine()으로 검색 및 답변 생성 준비.\n",
    "쿼리 실행: query_engine.query()로 질문 처리.\n",
    "응답 출력: response.response로 답변 출력, response.source_nodes로 검색된 원본 확인.\n",
    "```\n",
    "\n",
    "\n",
    "### 1. 파일 준비\n",
    " .txt, .pdf, .doc 등 다양한 형식의 파일을 특정 디렉토리(예: data 또는 sample_docs)에 준비합니다.\n",
    " 예: SimpleDirectoryReader(\"data\") 또는 SimpleDirectoryReader(\"sample_docs\")로 디렉토리를 지정.\n",
    "\n",
    "### 2. 파일 읽기\n",
    " SimpleDirectoryReader를 사용해 파일을 읽고 Document 객체로 변환합니다.\n",
    " recursive=True로 설정하면 하위 폴더도 포함하며, required_exts로 특정 파일 확장자(예: .txt, .pdf)만 로드 가능.\n",
    " 예:\n",
    "  documents = SimpleDirectoryReader(\"data\", recursive=True, required_exts=[\".txt\", \".pdf\"]).load_data()\n",
    "\n",
    "### 3. 텍스트 분할 및 노드 생성\n",
    " 문서를 작은 텍스트 청크로 나누고 Node 객체로 변환합니다. 검색과 임베딩을 효율적으로 처리하기 위해 필요.\n",
    " 사용 가능한 파서:\n",
    "   SimpleNodeParser: 고정된 크기(예: chunk_size=80)로 분할.\n",
    "   TokenTextSplitter: 토큰 단위로 분할.\n",
    "   SentenceSplitter: 문장 단위로 분할.\n",
    "   SemanticSplitterNodeParser: 의미 단위로 분할(임베딩 모델 필요).\n",
    " 예:\n",
    "  parser = SimpleNodeParser(chunk_size=80, chunk_overlap=0)\n",
    "  nodes = parser.get_nodes_from_documents(documents)\n",
    " 참고: 이 단계에서는 텍스트 분할만 수행되며, 벡터 임베딩은 이루어지지 않습니다.\n",
    "\n",
    "### 4. 인덱스 생성\n",
    " VectorStoreIndex를 사용해 분할된 노드(또는 문서)를 벡터화하고 인덱스를 생성.\n",
    " 텍스트 청크는 임베딩 모델(기본적으로 OpenAI text-embedding-ada-002)로 벡터화되어 인덱스에 저장.\n",
    " from_documents를 사용하면 문서를 직접 전달해 내부적으로 분할 및 임베딩 수행. 또는 nodes를 전달해 미리 분할된 노드 사용.\n",
    " 예:\n",
    "  index = VectorStoreIndex.from_documents(documents)\n",
    "  또는\n",
    "  index = VectorStoreIndex(nodes)\n",
    "\n",
    "### 5. 쿼리 엔진 생성\n",
    " 인덱스를 기반으로 쿼리 엔진을 생성하여 검색 및 답변 생성 준비.\n",
    " similarity_top_k로 검색 결과 수 조정, node_postprocessors로 후처리 설정(예: 유사도 임계값) 가능.\n",
    " 예:\n",
    "  query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "### 6. 쿼리 실행\n",
    " query_engine.query()를 사용해 사용자의 질문(쿼리)을 처리.\n",
    " 쿼리는 인덱스에서 유사한 노드를 검색한 후, 지정된 LLM(예: gpt-4o-mini)을 사용해 답변 생성.\n",
    " 예:\n",
    "  response = query_engine.query(\"이 영화의 반전은 무엇인가요?\")\n",
    "\n",
    "### 7. 응답 출력\n",
    " 쿼리 결과는 response 객체에 저장되며, response.response로 최종 답변 텍스트를 얻거나, response.source_nodes로 검색된 원본 노드 확인 가능.\n",
    " 예:\n",
    "  print(response.response)\n",
    "  for node in response.source_nodes:\n",
    "      print(node.text)\n",
    "\n",
    "### 추가 세부 사항\n",
    " 임베딩 모델: 인덱스 생성 시 텍스트 청크는 자동으로 벡터화. 기본적으로 OpenAI 임베딩 API 사용, HuggingFaceEmbedding 같은 로컬 모델로 변경 가능.\n",
    "  from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "  Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    " LLM 설정: 쿼리 엔진에서 답변 생성 시 사용되는 LLM은 Settings.llm 또는 llm 매개변수로 지정. 예: gpt-4o-mini 사용, 시스템 프롬프트로 \"반드시 한국어로 답변하세요\" 설정 가능.\n",
    " 벡터 저장소: 기본적으로 인메모리 벡터 저장소 사용, ChromaVectorStore 같은 외부 벡터 DB로 영구 저장 가능.\n",
    "\n",
    "### 정리\n",
    " 3번(텍스트 분할): 벡터 임베딩이 아닌 텍스트 분할만 수행.\n",
    " 4번(인덱스 생성): 텍스트 청크가 벡터로 변환되어 인덱스에 저장.\n",
    " 7번(응답 출력): response.response 외에도 response.source_nodes로 검색된 원본 텍스트 확인 가능.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex 워크플로우 절차\n",
    "작성일: 2025년 8월 26일\n",
    "\n",
    "1. 파일 준비\n",
    " .txt, .pdf, .doc 등 다양한 형식의 파일을 특정 디렉토리(예: data 또는 sample_docs)에 준비합니다.\n",
    " 예: SimpleDirectoryReader(\"data\") 또는 SimpleDirectoryReader(\"sample_docs\")로 디렉토리를 지정.\n",
    "\n",
    "2. 파일 읽기\n",
    " SimpleDirectoryReader를 사용해 파일을 읽고 Document 객체로 변환합니다.\n",
    " recursive=True로 설정하면 하위 폴더도 포함하며, required_exts로 특정 파일 확장자(예: .txt, .pdf)만 로드 가능.\n",
    " 예:\n",
    "  documents = SimpleDirectoryReader(\"data\", recursive=True, required_exts=[\".txt\", \".pdf\"]).load_data()\n",
    "\n",
    "3. 텍스트 분할 및 노드 생성\n",
    " 문서를 작은 텍스트 청크로 나누고 Node 객체로 변환합니다. 검색과 임베딩을 효율적으로 처리하기 위해 필요.\n",
    " 사용 가능한 파서:\n",
    "   SimpleNodeParser: 고정된 크기(예: chunk_size=80)로 분할.\n",
    "   TokenTextSplitter: 토큰 단위로 분할.\n",
    "   SentenceSplitter: 문장 단위로 분할.\n",
    "   SemanticSplitterNodeParser: 의미 단위로 분할(임베딩 모델 필요).\n",
    " 예:\n",
    "  parser = SimpleNodeParser(chunk_size=80, chunk_overlap=0)\n",
    "  nodes = parser.get_nodes_from_documents(documents)\n",
    " 이 단계에서는 텍스트 분할만 수행되며, 벡터 임베딩은 이루어지지 않음.\n",
    "\n",
    "4. 인덱스 생성\n",
    " VectorStoreIndex를 사용해 분할된 노드(또는 문서)를 벡터화하고 인덱스를 생성.\n",
    " 텍스트 청크는 임베딩 모델(기본적으로 OpenAI textembeddingada002)로 벡터화되어 인덱스에 저장.\n",
    " from_documents를 사용하면 문서를 직접 전달해 내부적으로 분할 및 임베딩 수행. 또는 nodes를 전달해 미리 분할된 노드 사용.\n",
    " 예:\n",
    "  index = VectorStoreIndex.from_documents(documents)\n",
    "  또는\n",
    "  index = VectorStoreIndex(nodes)\n",
    "\n",
    "5. 쿼리 엔진 생성\n",
    " 인덱스를 기반으로 쿼리 엔진을 생성하여 검색 및 답변 생성 준비.\n",
    " similarity_top_k로 검색 결과 수 조정, node_postprocessors로 후처리 설정(예: 유사도 임계값) 가능.\n",
    " 예:\n",
    "  query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "6. 쿼리 실행\n",
    " query_engine.query()를 사용해 사용자의 질문(쿼리)을 처리.\n",
    " 쿼리는 인덱스에서 유사한 노드를 검색한 후, 지정된 LLM(예: gpt4omini)을 사용해 답변 생성.\n",
    " 예:\n",
    "  response = query_engine.query(\"이 영화의 반전은 무엇인가요?\")\n",
    "\n",
    "7. 응답 출력\n",
    " 쿼리 결과는 response 객체에 저장되며, response.response로 최종 답변 텍스트를 얻거나, response.source_nodes로 검색된 원본 노드 확인 가능.\n",
    " 예:\n",
    "  print(response.response)\n",
    "  for node in response.source_nodes:\n",
    "      print(node.text)\n",
    "\n",
    "추가 세부 사항\n",
    " 임베딩 모델: 인덱스 생성 시 텍스트 청크는 자동으로 벡터화. 기본적으로 OpenAI 임베딩 API 사용, HuggingFaceEmbedding 같은 로컬 모델로 변경 가능.\n",
    "  예:\n",
    "  from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "  Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentencetransformers/allMiniLML6v2\")\n",
    " LLM 설정: 쿼리 엔진에서 답변 생성 시 사용되는 LLM은 Settings.llm 또는 llm 매개변수로 지정. 예: gpt4omini 사용, 시스템 프롬프트로 \"반드시 한국어로 답변하세요\" 설정 가능.\n",
    " 벡터 저장소: 기본적으로 인메모리 벡터 저장소 사용, ChromaVectorStore 같은 외부 벡터 DB로 영구 저장 가능.\n",
    "\n",
    "정리\n",
    " 3번(텍스트 분할): 벡터 임베딩이 아닌 텍스트 분할만 수행.\n",
    " 4번(인덱스 생성): 텍스트 청크가 벡터로 변환되어 인덱스에 저장.\n",
    " 7번(응답 출력): response.response 외에도 response.source_nodes로 검색된 원본 텍스트 확인 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-index\n",
    "pip install ipykernel\n",
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv  import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "!llamaindex-cli download-llamadataset PaulGrahamEssayDataset --download-dir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 09:48:10,404 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 09:48:13,157 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 09:48:16,664 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작가는 청소년 시절에 회화를 감상하는 것을 좋아했지만, 그것을 스스로 그릴 수 있는 능력에 대해 의심을 품었습니다. 그는 회화를 만들 수 있는 사람들을 다른 종류의 존재로 여겼으며, 회화를 만들 수 있는 능력에 대해 거의 기적적으로 생각했습니다. 이후 하버드에서 미술 수업을 듣기 시작하면서 예술가로서의 길을 모색하기 시작했습니다.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"작가의 청소년 시절은 어떠했나요?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작가는 청소년 시절에 회화를 감상하는 것을 좋아했지만, 그것을 스스로 그릴 수 있는 능력에 대해 의심을 품었습니다. 그는 회화를 만들 수 있는 사람들을 다른 종류의 존재로 여겼으며, 회화를 만들 수 있는 능력에 대해 거의 기적적으로 생각했습니다. 이후 하버드에서 미술 수업을 듣기 시작하면서 예술가로서의 길을 모색하기 시작했습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'작가는 청소년 시절에 회화를 감상하는 것을 좋아했지만, 그것을 스스로 그릴 수 있는 능력에 대해 의심을 품었습니다. 그는 회화를 만들 수 있는 사람들을 다른 종류의 존재로 여겼으며, 회화를 만들 수 있는 능력에 대해 거의 기적적으로 생각했습니다. 이후 하버드에서 미술 수업을 듣기 시작하면서 예술가로서의 길을 모색하기 시작했습니다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorStoreIndex.from_documents(documents)를 호출할 때, LlamaIndex는 내부적으로 문서의 텍스트를 자동으로 벡터화하여 저장합니다. 따라서 별도로 벡터로 변환하는 과정을 수동으로 수행할 필요는 없습니다. 이 과정은 다음과 같이 진행됩니다:\n",
    "\n",
    "텍스트 분할: SimpleDirectoryReader로 로드된 문서(documents)는 먼저 내부적으로 텍스트 청크(chunk)로 분할됩니다. 이는 SimpleNodeParser 또는 설정된 다른 파서(예: SentenceSplitter)를 통해 수행되며, 기본적으로 적절한 크기의 텍스트 조각으로 나뉩니다.\n",
    "\n",
    "임베딩 생성: 분할된 각 텍스트 청크는 LlamaIndex에서 설정된 임베딩 모델(기본적으로 OpenAI의 text-embedding-ada-002와 같은 모델)을 사용하여 벡터로 변환됩니다. 이 벡터는 텍스트의 의미를 숫자 형태로 표현한 것으로, 검색 및 유사도 계산에 사용됩니다.\n",
    "LlamaIndex는 기본적으로 OpenAI의 text-embedding-ada-002 모델을 사용하지만, Settings.embed_model을 통해 다른 모델(예: HuggingFace 모델)을 지정할 수 있습니다.\n",
    "\n",
    "벡터 저장: 변환된 벡터는 VectorStoreIndex에 저장됩니다. 이 인덱스는 벡터와 원본 텍스트, 그리고 메타데이터를 함께 저장하여 나중에 쿼리 시 효율적으로 검색할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 2장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install llama-index==0.11.11 -q\n",
    "# !pip install docx2txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 데이터 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 데이터 리더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 14:16:21,827 - WARNING - Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"sample_docs\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document 리스트의 각 요소에 접근하여 내용 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(document.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 14:16:24,027 - WARNING - Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    \"sample_docs\", recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)  # 4개의 문서가 로드됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "This is the content of file 1.\n",
      "Document 2:\n",
      "This is the content of file 2.\n",
      "Document 3:\n",
      "This is the content of ﬁle 3. \n",
      "Document 4:\n",
      "This is the content of file 4.\n",
      "Document 5:\n",
      "This is the content of file 5.\n"
     ]
    }
   ],
   "source": [
    "# Document 리스트의 각 요소에 접근하여 내용 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(document.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 14:16:27,864 - WARNING - Ignoring wrong pointing object 6 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    \"sample_docs\",\n",
    "    required_exts=[\".txt\", \".pdf\"],\n",
    "    recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "This is the content of file 1.\n",
      "Document 2:\n",
      "This is the content of ﬁle 3. \n",
      "Document 3:\n",
      "This is the content of file 4.\n"
     ]
    }
   ],
   "source": [
    "# Document 리스트의 각 요소에 접근하여 내용 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(document.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 metadata:\n",
      "{'file_path': 'c:\\\\Users\\\\com\\\\llama\\\\llama-index\\\\ch01\\\\sample_docs\\\\file1.txt', 'file_name': 'file1.txt', 'file_type': 'text/plain', 'file_size': 30, 'creation_date': '2025-08-26', 'last_modified_date': '2025-08-25'}\n",
      "Document 2 metadata:\n",
      "{'page_label': '1', 'file_name': 'file3.pdf', 'file_path': 'c:\\\\Users\\\\com\\\\llama\\\\llama-index\\\\ch01\\\\sample_docs\\\\file3.pdf', 'file_type': 'application/pdf', 'file_size': 9283, 'creation_date': '2025-08-26', 'last_modified_date': '2025-08-25'}\n",
      "Document 3 metadata:\n",
      "{'file_path': 'c:\\\\Users\\\\com\\\\llama\\\\llama-index\\\\ch01\\\\sample_docs\\\\sub_sample_docs\\\\file4.txt', 'file_name': 'file4.txt', 'file_type': 'text/plain', 'file_size': 30, 'creation_date': '2025-08-26', 'last_modified_date': '2025-08-25'}\n"
     ]
    }
   ],
   "source": [
    "# document 리스트의 각 요소에 접근하여 메타데이터를 출력\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1} metadata:\")\n",
    "    print(document.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 데이터 커넥터 \n",
    "- 데이터베이스 연결\n",
    "- 커넥터는 driver 와 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index-readers-database==0.3.0 -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymysql\n",
    "# !pip install cryptography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_db와 users 테이블 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# MySQL 접속 및 DB/테이블 생성\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(host='localhost', user='root', password='root', port=3306)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"CREATE DATABASE IF NOT EXISTS test_db;\")\n",
    "cur.execute(\"USE test_db;\")\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS users1 (\n",
    "        id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        name VARCHAR(100) NOT NULL,\n",
    "        email VARCHAR(100) NOT NULL UNIQUE,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"test_db와 users 테이블 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from llama_index.readers.database import DatabaseReader\n",
    "\n",
    "# MySQL 연결 정보 직접 입력\n",
    "scheme = \"mysql+pymysql\"\n",
    "host = \"localhost\"\n",
    "password = \"user\"\n",
    "port = \"3306\"\n",
    "user = \"user\"\n",
    "dbname = \"test_db\"\n",
    "\n",
    "connection_string = f\"{scheme}://{user}:{password}@{host}:{port}/{dbname}\"\n",
    "engine = create_engine(connection_string)\n",
    "reader = DatabaseReader(sql_database=engine)\n",
    "\n",
    "# 데이터 로드\n",
    "query = \"SELECT name, age FROM users\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reader.load_data(query=query)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "ID e1dde335-247f-434f-b6c0-1a1f396b9b47\n",
      "Row name: Alice, age: 30\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "ID 9343ac81-9b9b-438e-8d09-5a565ddd540a\n",
      "Row name: Bob, age: 32\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "ID ff3cac07-45d3-4983-ad9d-765ac265495e\n",
      "Row name: Charlie, age: 33\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# documents 리스트의 각 요소 출력\n",
    "for idx, doc in enumerate(documents):\n",
    "    print(f\"Document {idx + 1}:\")\n",
    "    print(f\"ID {doc.id_}\")\n",
    "    print(f\"Row {doc.text}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key themes in this document include advocating for a strategy of making numerous smaller investments rather than a few large ones, supporting younger and more technically-oriented founders over those with MBAs, allowing founders to retain their positions as CEOs, and the value of giving talks as a method for sparking creativity and sharing knowledge.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 텍스트 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 문서와 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: f34ced02-88fd-4d28-908c-0696770d9acf\n",
      "Text: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는\n",
      "평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는\n",
      "오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다.\n",
      "결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# 텍스트 데이터를 기반으로 문서 생성\n",
    "document_text = \"영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다. 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\"\n",
    "document = Document(text=document_text)\n",
    "\n",
    "# 메타데이터 추가\n",
    "document.metadata = {'author': '영화 해설', 'subject': '기생충 줄거리'}\n",
    "\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Document 객체의 속성 종류</b>\n",
    "\n",
    "llama_index.core.Document 객체는 텍스트 데이터와 그와 관련된 메타데이터를 저장하는 데 사용됩니다. metadata는 이 Document 객체가 가지고 있는 여러 속성 중 하나이며, 사용자가 직접 추가하거나 수정할 수 있는 부분입니다.\n",
    "\n",
    "Document 객체의 주요 속성 종류는 다음과 같습니다.\n",
    "\n",
    "<b>text</b>: Document의 핵심이 되는 본문 텍스트입니다. 위 예시에서는 '영화 '기생충'은...'으로 시작하는 줄거리 내용이 이 속성에 해당합니다.\n",
    "\n",
    "<b>metadata</b>: Document에 대한 추가 정보를 담는 딕셔너리(dictionary) 형태의 속성입니다. 이 정보는 문서 자체의 내용이 아닌, 문서와 관련된 부가적인 데이터를 저장하는 데 사용됩니다. 위 예시에서는 {'author': '영화 해설', 'subject': '기생충 줄거리'}와 같이 저자나 주제와 같은 정보를 저장했습니다.\n",
    "\n",
    "<b>id_</b>: 각 Document를 고유하게 식별하는 고유 ID입니다. 기본적으로는 UUID가 자동으로 생성되며, 사용자가 직접 지정할 수도 있습니다.\n",
    "\n",
    "<b>embedding</b>: Document 텍스트를 벡터로 변환한 임베딩(embedding) 값을 저장하는 속성입니다. 이 임베딩은 텍스트의 의미를 숫자로 표현한 것으로, 검색이나 유사도 비교에 활용됩니다. 기본적으로는 비어 있으며, 임베딩 모델을 통해 생성됩니다.\n",
    "\n",
    "excluded_embed_metadata_keys: 임베딩 생성 시 제외할 메타데이터 키 목록입니다.\n",
    "\n",
    "excluded_llm_metadata_keys: LLM(대규모 언어 모델)이 프롬프트로 사용할 때 제외할 메타데이터 키 목록입니다.\n",
    "\n",
    "hash: Document 내용의 변경 여부를 확인하는 데 사용되는 해시 값입니다.\n",
    "\n",
    "metadata_template: 메타데이터를 텍스트로 변환할 때 사용할 템플릿입니다.\n",
    "\n",
    "<b>text_template</b>: text 속성을 다른 텍스트와 결합할 때 사용할 템플릿입니다.\n",
    "\n",
    "정리하면, metadata는 Document 객체가 가진 여러 속성 중 하나이며, text, id_, embedding 등 다른 핵심 속성들과 함께 문서를 구성하는 중요한 요소입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.text\n",
    "document.metadata\n",
    "document.id_\n",
    "document.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서단위 검색과 결과 (분할없이)\n",
    "- 실제로는 llama-index 가 내부적으로 분할한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 15:08:02,378 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 15:08:02,849 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 15:08:03,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문서 단위 검색 결과\n",
      "문서 검색 응답: The movie's twist reveals the existence of a secret basement in the wealthy Park family's house, where a man has been hiding for a long time. This unexpected revelation leads to a series of events that culminate in a tragic ending.\n",
      "- 결과 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다. 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "# 한글 답변 설정\n",
    "llm = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "\t\t\t\t\t\t\tmodel=\"gpt-4o-mini\", \n",
    "\t\t\t\t\t\t\tsystem_prompt=\"반드시 한국어로 답변하세요.\")\n",
    "# Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0, system_prompt=\"항상 한국어로 답변하세요.\")\n",
    "\n",
    "\n",
    "# 텍스트 데이터를 기반으로 문서객체 생성\n",
    "document_text = \"영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다. 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\"\n",
    "document = Document(text=document_text)\n",
    "\n",
    "# 메타데이터 추가\n",
    "document.metadata = {'author': '영화 해설', 'subject': '기생충 줄거리'}\n",
    "\n",
    "\n",
    "# 문서 단위 검색을 위한 전체 문서 인덱스 생성\n",
    "full_doc_index = VectorStoreIndex.from_documents([document], llm=llm)\n",
    "\n",
    "\n",
    "# 검색 비교\n",
    "query_text = '이 영화의 반전은 무엇인가요?'\n",
    "\n",
    "\n",
    "## 문서 단위 검색\n",
    "doc_query_engine = full_doc_index.as_query_engine()\n",
    "doc_response = doc_query_engine.query(query_text)\n",
    "print(\"\\n문서 단위 검색 결과\")\n",
    "print(f\"문서 검색 응답: {doc_response.response}\")\n",
    "if doc_response.source_nodes:\n",
    "    for idx, document in enumerate(doc_response.source_nodes, start=1):\n",
    "        print(f\"- 결과 {idx}: {document.node.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SimpleNodeParser 이용 chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 노드 단위 검색과 결과\n",
    "- from llama_index.core.node_parser.text.sentence import SentenceSplitter\n",
    "- from llama_index.core.node_parser.text.token import TokenTextSplitter\n",
    "- from llama_index.core.node_parser import SemanticSplitterNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser.text.sentence import SentenceSplitter\n",
    "from llama_index.core.node_parser.text.token import TokenTextSplitter\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장단위 (마침표 기준) chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 SimpleNodeParser 를 이용한 텍스트 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "생성된 노드들\n",
      "노드 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다.\n",
      "메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 1}\n",
      "노드 2: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다.\n",
      "메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 2}\n",
      "노드 3: 박 사장네 집에는 비밀 지하실이 존재하며, 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다.\n",
      "메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 3}\n",
      "노드 4: 이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다. 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\n",
      "메타데이터: {'type': '영화 줄거리', 'genre': '드라마', 'node_id': 4}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser(chunk_size=80, chunk_overlap=0)\n",
    "nodes = parser.get_nodes_from_documents([document])  # document는 위에서 정의\n",
    "\n",
    "print(\"\\n생성된 노드들\")\n",
    "for idx, node in enumerate(nodes, start=1):\n",
    "    node.metadata = {'type': '영화 줄거리', 'genre': '드라마', 'node_id': idx}\n",
    "    print(f\"노드 {idx}: {node.text}\")\n",
    "    print(f\"메타데이터: {node.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 토큰 단위 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 토큰 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 \n",
      "\n",
      "Chunk 2: 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 \n",
      "\n",
      "Chunk 3: 쌓이며 긴장감이 점점 고조됩니다. 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 그러나 이 영화의 반전은 지하실에서 시작됩니다.\"\n",
    "\n",
    "\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(sample_text)\n",
    "print(\"=== 토큰 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(token_chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 문장 단위 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 문장 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 \n",
      "\n",
      "Chunk 2: 벌어지는 이야기입니다. \n",
      "\n",
      "Chunk 3: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. \n",
      "\n",
      "Chunk 4: 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser.text.sentence import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=50, chunk_overlap=0)\n",
    "\n",
    "sentence_chunks = splitter.split_text(sample_text)\n",
    "print(\"=== 문장 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(sentence_chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 문장 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 \n",
      "\n",
      "Chunk 2: 벌어지는 이야기입니다. \n",
      "\n",
      "Chunk 3: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. \n",
      "\n",
      "Chunk 4: 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터를 기반으로 문서객체 생성\n",
    "document_text = \"영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 \" \\\n",
    "\"취업하면서 벌어지는 이야기입니다. 처음에는 평화로워 보이지만, \" \\\n",
    "\"이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. 박 사장네 집에는 비밀 지하실이 존재하며,\" \\\n",
    "\" 그곳에는 오랫동안 숨어 살던 남자가 있다는 반전이 있습니다. \" \\\n",
    "\"이 사실을 알게 된 기우네 가족은 예상치 못한 위기를 맞이하게 됩니다.\" \\\n",
    "\" 결국 극한 상황에서 벌어지는 사건으로 인해 비극적인 결말로 이어집니다.\"\n",
    "document = Document(text=document_text)\n",
    "\n",
    "\n",
    "# NodeParser를 사용하여 문서를 작은 의미 단위(Node)로 분할\n",
    "parser = SentenceSplitter(chunk_size=100, chunk_overlap=0)\n",
    "nodes = parser.get_nodes_from_documents([document])\n",
    "\n",
    "\n",
    "\n",
    "# 노드 단위 검색을 위한 인덱스 생성\n",
    "llm = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "\t\t\t\t\t\t\tmodel=\"gpt-4o-mini\", \n",
    "\t\t\t\t\t\t\tsystem_prompt=\"반드시 한국어로 답변하세요.\")\n",
    "\n",
    "node_index = VectorStoreIndex(nodes, llm=llm)\n",
    "\n",
    "# 검색 비교\n",
    "query_text = '이 영화의 반전은 무엇인가요?'\n",
    "\n",
    "\n",
    "## 문장 단위 검색\n",
    "print(\"\\n문장 단위 분할 결과\")\n",
    "node_query_engine = node_index.as_query_engine()\n",
    "node_response = node_query_engine.query(query_text)\n",
    "print(f\"노드 검색 응답: {node_response.response}\")\n",
    "if node_response.source_nodes:\n",
    "    for idx, document in enumerate(node_response.source_nodes, start=1):\n",
    "        print(f\"- 결과 노드 {idx}: {document.node.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노드 검색 응답: The movie's twist revolves around a series of events that unfold in extreme circumstances, leading to a tragic ending.\n"
     ]
    }
   ],
   "source": [
    "print(f\"노드 검색 응답: {node_response.response}\") # 한국어가 아니네.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 의미 단위 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 의미 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. \n",
      "\n",
      "Chunk 2: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. \n",
      "\n",
      "Chunk 3: 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "chunks = splitter.sentence_splitter(sample_text)\n",
    "print(\"=== 의미 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.5 허깅페이스 임베딩 이용하기\n",
    "```\n",
    "임베더를 로컬에 다운받아 사용. 따라서 무료이고 빠르다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-readers-database 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.13.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 설치에 3분 걸림\n",
    "!pip install transformers \n",
    "!pip install llama-index-embeddings-huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 15:38:26,774 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 의미 기반 분할 결과 ===\n",
      "Chunk 1: 영화 '기생충'은 가난한 가족인 기우네가 부유한 박 사장네 집에 하나씩 취업하면서 벌어지는 이야기입니다. \n",
      "\n",
      "Chunk 2: 처음에는 평화로워 보이지만, 이들의 거짓말이 쌓이며 긴장감이 점점 고조됩니다. \n",
      "\n",
      "Chunk 3: 그러나 이 영화의 반전은 지하실에서 시작됩니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "chunks = splitter.sentence_splitter(sample_text)\n",
    "print(\"=== 의미 기반 분할 결과 ===\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk.strip(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 인덱싱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 벡터저장 인덱스\n",
    "\n",
    "임베딩 생성은 Settings.embed_model에 지정된 모델을 통해 수행됩니다.\n",
    "\n",
    "VectorStoreIndex는 내부적으로 Node 객체의 text를 임베딩 모델(기본적으로 OpenAI의 text-embedding-ada-002)로 벡터화합니다.\n",
    "\n",
    "이 과정에서 각 Node의 텍스트는 벡터로 변환되어 Node.embedding 속성에 저장되며, 인덱스에 포함됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 15:53:44,997 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader('data2')\n",
    "documents = reader.load_data()\n",
    "\n",
    "# Document 객체를 전달하여 인덱스 생성\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Top-K 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 15:53:58,889 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 15:53:59,794 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[검색된 상위 3개 문서]\n",
      "\n",
      "[문서 1]\n",
      "고양이는 물을 충분히 마셔야 합니다. 수분이 부족하면 신장 문제가 발생할 수 있습니다.\n",
      "건식 사료보다 습식 사료가 수분 공급에 도움이 됩니다.\n",
      "\n",
      "[문서 2]\n",
      "고양이는 육식 동물입니다. 주로 고기, 생선, 그리고 가공된 고양이 사료를 먹습니다.\n",
      "특히, 단백질이 풍부한 음식을 선호하며, 탄수화물 섭취는 적은 편입니다.\n",
      "\n",
      "[문서 3]\n",
      "고양이는 초콜릿, 양파, 마늘 같은 음식은 먹으면 안 됩니다.\n",
      "특히, 초콜릿에 포함된 테오브로민 성분은 고양이에게 치명적일 수 있습니다.\n",
      "\n",
      "[답변]\n",
      "고양이에게 수분 공급이 중요한 이유는 신장 문제를 예방하기 위해서입니다.\n"
     ]
    }
   ],
   "source": [
    "# 상위 3개의 결과를 반환\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "response = query_engine.query(\"고양이에게 수분 공급이 중요한 이유는?\")\n",
    "\n",
    "print(\"[검색된 상위 3개 문서]\")\n",
    "\n",
    "for idx, node in enumerate(response.source_nodes):\n",
    "    print(f\"\\n[문서 {idx+1}]\\n{node.text}\")\n",
    "print(\"\\n[답변]\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 VectorDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromadb 설치 1분정도\n",
    "!pip install chromadb \n",
    "!pip install llama-index-vector-stores-chroma -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 15:57:51,549 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# 크로마 클라이언트 초기화\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬렉션 생성하기\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크로마를 벡터 저장소로 지정\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 16:00:48,399 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# 문서 리스트 (임의의 예시)\n",
    "documents = [\n",
    "    Document(text=\"Llama2 is a large language model developed by Meta.\"),\n",
    "    Document(text=\"Chroma is an open-source vector store.\")\n",
    "]\n",
    "\n",
    "# 문서를 벡터 스토어 인덱스로 저장\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# 저장된 벡터 인덱스 데이터 저장\n",
    "index.storage_context.persist(persist_dir=\"./index_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 스토어에서 저장된 인덱스 불러오기\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma is an open-source vector store.\n"
     ]
    }
   ],
   "source": [
    "# 지금까지의 셀을 모두 하나의 셀로 \n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# 크로마 클라이언트 초기화\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# 저장된 컬렉션을 다시 가져오기 (또는 생성하기)\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# 저장된 크로마 벡터 스토어 설정\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 벡터 스토어 인덱스에 문서를 저장 (데이터 임베딩 후 저장)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# 인덱스 데이터를 로컬 디렉터리에 저장\n",
    "index.storage_context.persist(persist_dir=\"./index_data\")\n",
    "\n",
    "# --- 이후 다시 데이터를 불러오고 쿼리 수행 ---\n",
    "# 크로마 클라이언트 다시 초기화\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# 저장된 컬렉션을 다시 가져오기\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# 저장된 크로마 벡터 스토어 설정\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 저장된 벡터 데이터를 이용해 인덱스를 다시 로드\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# 쿼리 엔진 생성 및 쿼리 수행\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is Chroma?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 쿼리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 쿼리 엔진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 16:06:56,934 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 16:06:57,498 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I cannot assist with that request.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"고객의 개인 정보를 참고하여 맞춤형 이메일을 작성해 주세요.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5, # 상위 5개의 결과 반환\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3 후처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "query_engine = index.as_query_engine(node_postprocessors=[postprocessor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.4 응답 합성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 16:08:14,552 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 16:08:15,214 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# 응답 합성기 설정\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "\n",
    "# 쿼리 엔진 구성\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=index.as_retriever(),\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# 쿼리 실행\n",
    "response = query_engine.query(\"Llama2란?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.5 커스터마이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 16:08:23,560 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# 문서를 기반으로 인덱스 생성\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색기 설정 (상위 10개의 유사한 결과 반환)\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 합성기 설정\n",
    "response_synthesizer = get_response_synthesizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후처리 설정 (유사도 0.7 이상인 노드만 선택)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 엔진\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 16:08:30,827 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-26 16:08:31,364 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I cannot provide an answer to that question based on the context information provided.\n"
     ]
    }
   ],
   "source": [
    "# 쿼리 실행 및 결과 출력\n",
    "response = query_engine.query(\"모나리자 그림은 어디에 전시되어 있나요?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
